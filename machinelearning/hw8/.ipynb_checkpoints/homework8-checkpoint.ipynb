{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15402903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "patientdata = []\n",
    "for filenum in range(1, 71):\n",
    "    patientnum = 'Patient Number ' + str(filenum)\n",
    "    if filenum < 10:\n",
    "        with open(\"C:\\\\Users\\\\Ayman\\\\Documents\\\\machinelearning\\\\hw5\\\\diabetes-data\\\\Diabetes-Data\\\\data-0\"+str(filenum), \"r\") as file:\n",
    "            for line in file:\n",
    "                fourth = str(line)\n",
    "                date = fourth[0:fourth.index('\\t')]\n",
    "                fourth = fourth.replace(date, '')\n",
    "                fourth = fourth.replace('\\t', '', 1)\n",
    "                time = fourth[0:fourth.index('\\t')]\n",
    "                fourth = fourth.replace(time, '')\n",
    "                fourth = fourth.replace('\\t', '', 1)\n",
    "                third = fourth[0:fourth.index('\\t')]\n",
    "                patientdata.append(float(third))\n",
    "    else:\n",
    "        with open(\"C:\\\\Users\\\\Ayman\\\\Documents\\\\machinelearning\\\\hw5\\\\diabetes-data\\\\Diabetes-Data\\\\data-\"+str(filenum), \"r\") as file:\n",
    "            for line in file:\n",
    "                fourth = str(line)\n",
    "                date = fourth[0:fourth.index('\\t')]\n",
    "                fourth = fourth.replace(date, '')\n",
    "                fourth = fourth.replace('\\t', '', 1)\n",
    "                time = fourth[0:fourth.index('\\t')]\n",
    "                fourth = fourth.replace(time, '')\n",
    "                fourth = fourth.replace('\\t', '', 1)\n",
    "                third = fourth[0:fourth.index('\\t')]\n",
    "                patientdata.append(float(third))\n",
    "\n",
    "df = pd.DataFrame(patientdata, columns=['col3'])#one hot encoding for the dataframe\n",
    "onehotencoded = pd.get_dummies(df['col3'], prefix='col3')\n",
    "datahot = pd.concat([df, onehotencoded], axis=1)\n",
    "\n",
    "#df_encoded.to_csv(\"C:\\\\Users\\\\Ayman\\\\Documents\\\\machinelearning\\\\hw4\\\\test_encoded.csv\", index=False)\n",
    "#print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c57fa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[58  0  0 ...  0  0  0]\n",
      " [33  0  0 ...  0  0  0]\n",
      " [34  0  0 ...  0  0  0]\n",
      " ...\n",
      " [34  0  0 ...  0  0  0]\n",
      " [34  0  0 ...  0  0  0]\n",
      " [34  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "datahot = datahot.astype(int)#convering the booleans to int for 0 and 1\n",
    "print(datahot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421934ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Ngram_1  Ngram_2  Ngram_3  Ngram_4  Ngram_5\n",
      "0         58.0     33.0     34.0     62.0     33.0\n",
      "1         33.0     34.0     62.0     33.0     48.0\n",
      "2         34.0     62.0     33.0     48.0     58.0\n",
      "3         62.0     33.0     48.0     58.0     33.0\n",
      "4         33.0     48.0     58.0     33.0     34.0\n",
      "...        ...      ...      ...      ...      ...\n",
      "29321     33.0     34.0     48.0     58.0     33.0\n",
      "29322     34.0     48.0     58.0     33.0     34.0\n",
      "29323     48.0     58.0     33.0     34.0     34.0\n",
      "29324     58.0     33.0     34.0     34.0     34.0\n",
      "29325     33.0     34.0     34.0     34.0     34.0\n",
      "\n",
      "[29326 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "def generatengrams(data, n):\n",
    "    ngrams_list = [tuple(data[i:i+n]) for i in range(len(data)-n+1)]\n",
    "    return ngrams_list\n",
    "\n",
    "ngrams = generatengrams(patientdata, n)#generating ngrams and placing those ngrams into the dataframe\n",
    "\n",
    "ngramsdf = pd.DataFrame(ngrams, columns=[f'Ngram_{i+1}' for i in range(n)])\n",
    "\n",
    "print(ngramsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786afc32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000e+00 0.000e+00 3.518e+03 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.053e+03 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 2.190e+02 2.000e+01 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.390e+02 0.000e+00 0.000e+00\n",
      " 3.310e+02 0.000e+00 3.830e+03 0.000e+00 3.400e+01 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.209e+03\n",
      " 0.000e+00 0.000e+00 0.000e+00 3.300e+01 0.000e+00 9.800e+01 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 6.800e+01\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.600e+02 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 2.771e+03 0.000e+00 1.190e+02 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 9.518e+03 1.000e+00 3.160e+03 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.894e+03\n",
      " 0.000e+00 1.540e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "def hashing(patientdata, n):\n",
    "    hashed = np.zeros(n)\n",
    "    for x in patientdata:\n",
    "        i = hash(str(x)) % n\n",
    "        hashed[i] += 1\n",
    "    return hashed\n",
    "hasheddata = hashing(patientdata, 100)\n",
    "print(hasheddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(datahot.drop(columns=['col3']), datahot['col3'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21dcf5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15847, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12782, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 9981, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7452, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5221, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2231, Classes: [60]\n",
      "Depth 4, Number of instances: 2529, Classes: [62]\n",
      "Depth 3, Number of instances: 2801, Classes: [58]\n",
      "Depth 2, Number of instances: 3065, Classes: [34]\n",
      "Depth 1, Number of instances: 7617, Classes: [33]\n",
      "Decision Tree Accuracy: 0.8416297306512104\n",
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15923, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12908, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 10083, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7558, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5306, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2252, Classes: [60]\n",
      "Depth 4, Number of instances: 2525, Classes: [62]\n",
      "Depth 3, Number of instances: 2825, Classes: [58]\n",
      "Depth 2, Number of instances: 3015, Classes: [34]\n",
      "Depth 1, Number of instances: 7541, Classes: [33]\n",
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15884, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12891, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 10040, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7530, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5184, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2346, Classes: [60]\n",
      "Depth 4, Number of instances: 2510, Classes: [62]\n",
      "Depth 3, Number of instances: 2851, Classes: [58]\n",
      "Depth 2, Number of instances: 2993, Classes: [34]\n",
      "Depth 1, Number of instances: 7580, Classes: [33]\n",
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15821, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12771, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 9993, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7514, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5207, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2307, Classes: [60]\n",
      "Depth 4, Number of instances: 2479, Classes: [62]\n",
      "Depth 3, Number of instances: 2778, Classes: [58]\n",
      "Depth 2, Number of instances: 3050, Classes: [34]\n",
      "Depth 1, Number of instances: 7643, Classes: [33]\n",
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15916, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12819, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 9975, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7494, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5169, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2325, Classes: [60]\n",
      "Depth 4, Number of instances: 2481, Classes: [62]\n",
      "Depth 3, Number of instances: 2844, Classes: [58]\n",
      "Depth 2, Number of instances: 3097, Classes: [34]\n",
      "Depth 1, Number of instances: 7548, Classes: [33]\n",
      "Depth 0, Number of instances: 23464, Classes: [ 0 33 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 1, Number of instances: 15769, Classes: [ 0 34 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 2, Number of instances: 12756, Classes: [ 0 35 48 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 3, Number of instances: 9901, Classes: [ 0 35 48 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 4, Number of instances: 7384, Classes: [ 0 35 48 56 57 59 60 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 5222, Classes: [ 0 35 48 56 57 59 61 63 64 65 66 67 68 69 70 71 72]\n",
      "Depth 5, Number of instances: 2162, Classes: [60]\n",
      "Depth 4, Number of instances: 2517, Classes: [62]\n",
      "Depth 3, Number of instances: 2855, Classes: [58]\n",
      "Depth 2, Number of instances: 3013, Classes: [34]\n",
      "Depth 1, Number of instances: 7695, Classes: [33]\n",
      "Random Forest Accuracy: 0.8416297306512104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(datahot.drop(columns=['col3']), datahot['col3'], test_size=0.2, random_state=42)\n",
    "\n",
    "def ginimpurity(labels):\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "    probabilities = counts / len(labels)#gini impurity calculation\n",
    "    gini = 1 - np.sum(probabilities**2)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def informationgain(groups, y):\n",
    "    total = sum(len(group) for group in groups)\n",
    "    ig = ginimpurity(y)#uses impurity to calculate the id\n",
    "    for group in groups:\n",
    "        proportion = len(group) / total\n",
    "        ig -= proportion * ginimpurity(group)\n",
    "    return ig\n",
    "\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_X, left_y, right_X, right_y = [], [], [], []\n",
    "    for i, val in enumerate(X[:, feature_index]):\n",
    "        if val <= threshold:#splitting it based on features\n",
    "            left_X.append(X[i])\n",
    "            left_y.append(y[i])\n",
    "        else:\n",
    "            right_X.append(X[i])\n",
    "            right_y.append(y[i])\n",
    "    return np.array(left_X), np.array(left_y), np.array(right_X), np.array(right_y)\n",
    "\n",
    "\n",
    "def bestsplit(X, y):\n",
    "    best_feature_index, best_threshold, best_info_gain = None, None, 0\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            groups = split(X, y, feature_index, threshold)\n",
    "            info_gain = informationgain([groups[1], groups[3]], y)#takes it and compares and selects the best one\n",
    "            if info_gain > best_info_gain:\n",
    "                best_feature_index, best_threshold, best_info_gain = feature_index, threshold, info_gain\n",
    "    return best_feature_index, best_threshold, best_info_gain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_tree(X, y, max_depth, depth):\n",
    "    print(f\"Depth {depth}, Number of instances: {len(X)}, Classes: {np.unique(y)}\")\n",
    "    if depth == max_depth or len(np.unique(y)) == 1:\n",
    "        return Node(value=np.bincount(y).argmax())\n",
    "    \n",
    "    best_feature_index, best_threshold, best_info_gain = bestsplit(X, y)\n",
    "    if best_info_gain == 0:\n",
    "        return Node(value=np.bincount(y).argmax())\n",
    "    \n",
    "    left_X, left_y, right_X, right_y = split(X, y, best_feature_index, best_threshold)\n",
    "    left_subtree = build_tree(left_X, left_y, max_depth, depth + 1)\n",
    "    right_subtree = build_tree(right_X, right_y, max_depth, depth + 1)\n",
    "    \n",
    "    return Node(best_feature_index, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "def predict(node, X):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if X[node.feature_index] <= node.threshold:\n",
    "        return predict(node.left, X)\n",
    "    else:\n",
    "        return predict(node.right, X)\n",
    "\n",
    "\n",
    "decision_tree = build_tree(xtrain.values, ytrain.values, 5, 0)\n",
    "predictions = []\n",
    "for i in range(len(xtest)):\n",
    "    predictions.append(predict(decision_tree, xtest.iloc[i]))\n",
    "accuracy = sum(predictions == ytest.values) / len(ytest)\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "################################################################\n",
    "def random_forest(X_train, y_train, n_estimators, max_depth):\n",
    "    forest = []\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_sampled = X_train[sample_indices]\n",
    "        y_sampled = y_train[sample_indices]\n",
    "        tree = build_tree(X_sampled, y_sampled, max_depth, 0)\n",
    "        forest.append(tree)\n",
    "    return forest\n",
    "forest = random_forest(xtrain.values, ytrain.values, 5, 5)\n",
    "\n",
    "#make predictions using random forest\n",
    "def predict_random_forest(forest, X_test):\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        instance_predictions = [predict(tree, X_test.iloc[i]) for tree in forest]\n",
    "        majority_vote = np.bincount(instance_predictions).argmax()\n",
    "        predictions.append(majority_vote)\n",
    "    return predictions\n",
    "\n",
    "predictionsrf = predict_random_forest(forest, xtest)\n",
    "accuracy = sum(predictionsrf == ytest.values) / len(ytest)\n",
    "print(\"Random Forest Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
